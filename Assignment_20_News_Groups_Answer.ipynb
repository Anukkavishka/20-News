{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\comp\\comp.graphics\n",
      ".\\data\\comp\\comp.os.ms-windows.misc\n",
      ".\\data\\comp\\comp.sys.ibm.pc.hardware\n",
      ".\\data\\comp\\comp.sys.mac.hardware\n",
      ".\\data\\comp\\comp.windows.x\n",
      ".\\data\\rec\\rec.autos\n",
      ".\\data\\rec\\rec.motorcycles\n",
      ".\\data\\rec\\rec.sport.baseball\n",
      ".\\data\\rec\\rec.sport.hockey\n",
      ".\\data\\sci\\sci.crypt\n",
      ".\\data\\sci\\sci.electronics\n",
      ".\\data\\sci\\sci.med\n",
      ".\\data\\sci\\sci.space\n",
      ".\\data\\talk\\talk.politics.guns\n",
      ".\\data\\talk\\talk.politics.mideast\n",
      ".\\data\\talk\\talk.politics.misc\n",
      ".\\data\\talk\\talk.religion.misc\n",
      "11242\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from random import shuffle\n",
    "root='.\\\\data'\n",
    "root_content=os.listdir(root)\n",
    "classes =root_content\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "wnl = WordNetLemmatizer()\n",
    "#vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "dic = {}\n",
    "list_dic =[]\n",
    "###################################################################################\n",
    "def csv_header(dic,label,flag) :\n",
    "    \n",
    "    if flag== true :\n",
    "        csv_file=open(str(label)+\"_train.csv\",\"w\")\n",
    "    else :\n",
    "        csv_file=open(str(label)+\"_test.csv\",\"w\")\n",
    "    for w in dic:\n",
    "        csv_file.write(\"%s,\"%(w))\n",
    "    csv_file.write(\"class\\n\")\n",
    "    return csv_file\n",
    "    \n",
    "\n",
    "###################################################################################\n",
    "for element in classes :\n",
    "    path=os.path.join(root,element)\n",
    "    sub_folders=os.listdir(path)\n",
    "    file_dirc_list_one=[]        \n",
    "    for sub_elem in sub_folders :\n",
    "        sub_path=os.path.join(path,sub_elem)\n",
    "        sub_folder_content=[f for f in os.listdir(sub_path)]\n",
    "        print(sub_path)\n",
    "        for file in sub_folder_content:\n",
    "            file_dirc_list_one.append(join(sub_path, file)) \n",
    "        train_list, test_list = train_test_split( file_dirc_list_one,test_size=0.3 )#split files into train and test\n",
    "    \n",
    "    for file in train_list:\n",
    "        \n",
    "        data=open(file,'r').read()      \n",
    "        wordscontent_in_data=re.split('[^a-zA-Z]',data)              \n",
    "        words_in_file=[]\n",
    "\n",
    "        for word in wordscontent_in_data:\n",
    "            if word not in stopWords:\n",
    "                if (len(word)>1):\n",
    "                    add_w=wnl.lemmatize(word.strip().lower(),\"v\")\n",
    "                    words_in_file.append(add_w)\n",
    "                    \n",
    "                    \n",
    "        list_dic.append(words_in_file)\n",
    "    for w in list_dic:\n",
    "        if str(w) not in dic.keys():\n",
    "            dic[str(w)]=0\n",
    "\n",
    "print(len(dic))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\comp\\comp.graphics\n",
      ".\\data\\comp\\comp.os.ms-windows.misc\n",
      ".\\data\\comp\\comp.sys.ibm.pc.hardware\n",
      ".\\data\\comp\\comp.sys.mac.hardware\n",
      ".\\data\\comp\\comp.windows.x\n",
      "41954\n",
      ".\\data\\rec\\rec.autos\n",
      ".\\data\\rec\\rec.motorcycles\n",
      ".\\data\\rec\\rec.sport.baseball\n",
      ".\\data\\rec\\rec.sport.hockey\n",
      "55334\n",
      ".\\data\\sci\\sci.crypt\n",
      ".\\data\\sci\\sci.electronics\n",
      ".\\data\\sci\\sci.med\n",
      ".\\data\\sci\\sci.space\n",
      "70173\n",
      ".\\data\\talk\\talk.politics.guns\n",
      ".\\data\\talk\\talk.politics.mideast\n",
      ".\\data\\talk\\talk.politics.misc\n",
      ".\\data\\talk\\talk.religion.misc\n",
      "82797\n",
      "82797\n",
      ".\\data\\comp\\comp.graphics\n",
      ".\\data\\comp\\comp.os.ms-windows.misc\n",
      ".\\data\\comp\\comp.sys.ibm.pc.hardware\n",
      ".\\data\\comp\\comp.sys.mac.hardware\n",
      ".\\data\\comp\\comp.windows.x\n",
      ".\\data\\rec\\rec.autos\n",
      ".\\data\\rec\\rec.motorcycles\n",
      ".\\data\\rec\\rec.sport.baseball\n",
      ".\\data\\rec\\rec.sport.hockey\n",
      ".\\data\\sci\\sci.crypt\n",
      ".\\data\\sci\\sci.electronics\n",
      ".\\data\\sci\\sci.med\n",
      ".\\data\\sci\\sci.space\n",
      ".\\data\\talk\\talk.politics.guns\n",
      ".\\data\\talk\\talk.politics.mideast\n",
      ".\\data\\talk\\talk.politics.misc\n",
      ".\\data\\talk\\talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from random import shuffle\n",
    "root='.\\\\data'\n",
    "root_content=os.listdir(root)\n",
    "classes =root_content\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#import pandas as pd\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "wnl = WordNetLemmatizer()\n",
    "#vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "dic = {}\n",
    "list_dic=[]\n",
    "###################################################################################\n",
    "def csv_header(dic,label,flag) :\n",
    "    if flag == True :\n",
    "        csv_file=open(str(label)+\"_train.csv\",\"w\")\n",
    "    else :\n",
    "        csv_file=open(str(label)+\"_test.csv\",\"w\")\n",
    "    \n",
    "    for w in dic:\n",
    "        csv_file.write(\"%s,\"%(w))\n",
    "    csv_file.write(\"class\\n\")\n",
    "    return csv_file\n",
    "    \n",
    "\n",
    "###################################################################################\n",
    "def initialize_dic() :\n",
    "    global dic\n",
    "    \n",
    "    for element in classes :\n",
    "        path=os.path.join(root,element)\n",
    "        sub_folders=os.listdir(path)\n",
    "        file_dirc_list_one=[]        \n",
    "        for sub_elem in sub_folders :\n",
    "            sub_path=os.path.join(path,sub_elem)\n",
    "            sub_folder_content=[f for f in os.listdir(sub_path)]\n",
    "            print(sub_path)\n",
    "            for file in sub_folder_content:\n",
    "                file_dirc_list_one.append(join(sub_path, file)) \n",
    "            train_list, test_list = train_test_split( file_dirc_list_one,test_size=0.3 )#split files into train and test\n",
    "\n",
    "        for file in train_list:\n",
    "\n",
    "            data=open(file,'r').read()      \n",
    "            wordscontent_in_data=re.split('[^a-zA-Z]',data)              \n",
    "            \n",
    "\n",
    "            for word in wordscontent_in_data:\n",
    "                if word not in stopWords:\n",
    "                    if (len(word)>1):\n",
    "                        add_w=wnl.lemmatize(word.strip().lower(),\"v\")\n",
    "                        if add_w not in list_dic:\n",
    "                            list_dic.append(add_w)\n",
    "        print(len(list_dic))   \n",
    "    for w in list_dic:\n",
    "        if str(w) not in dic.keys():\n",
    "            dic[str(w)]=0\n",
    "    print(len(dic))\n",
    "        \n",
    "\n",
    "\n",
    "###################################################################################\n",
    "initialize_dic()\n",
    "\n",
    "\n",
    "\n",
    "for element in classes :\n",
    "    path=os.path.join(root,element)\n",
    "    sub_folders=os.listdir(path)\n",
    "    file_dirc_list=[]        \n",
    "    for sub_elem in sub_folders :\n",
    "        sub_path=os.path.join(path,sub_elem)\n",
    "        sub_folder_content=[f for f in os.listdir(sub_path)]\n",
    "        print(sub_path)\n",
    "        for file in sub_folder_content:\n",
    "            file_dirc_list.append(join(sub_path, file)) \n",
    "        train_list, test_list = train_test_split( file_dirc_list,test_size=0.3 )#split files into train and test\n",
    "\n",
    "#############################################################################################################    \n",
    "    #training \n",
    "    csv_file=csv_header(dic,element,True)\n",
    "    for file in train_list:\n",
    "        \n",
    "        data=open(file,'r').read()      \n",
    "        wordscontent_in_data=re.split('[^a-zA-Z]',data)              \n",
    "        words_in_file=[]\n",
    "\n",
    "        for word in wordscontent_in_data:\n",
    "            if word not in stopWords:\n",
    "                if (len(word)>1):\n",
    "                    words_in_file.append(wnl.lemmatize(word.strip().lower(),\"v\"))\n",
    "                        \n",
    "        doc_dic={}\n",
    "        for w in words_in_file:\n",
    "            if w not in doc_dic:\n",
    "                doc_dic[w]=1\n",
    "            else:\n",
    "                doc_dic[w]=doc_dic[w]+1\n",
    "        \n",
    "        for wd in dic:\n",
    "            if wd in doc_dic:\n",
    "                csv_file.write(\"%s,\"%(doc_dic[wd]))\n",
    "                \n",
    "            else:\n",
    "                csv_file.write(\"0,\")\n",
    "                \n",
    "        \n",
    "        if element=='comp':\n",
    "            csv_file.write(\"0\\n\")\n",
    "        elif element=='rec':\n",
    "            csv_file.write(\"1\\n\")\n",
    "        elif element=='sci':\n",
    "            csv_file.write(\"2\\n\")\n",
    "        elif element=='talk':\n",
    "            csv_file.write(\"3\\n\")\n",
    "        \n",
    "        #csv_file.write(\"\\n\")\n",
    "#############################################################################################################\n",
    "    #testing \n",
    "    csv_file=csv_header(dic,element,False)\n",
    "    for file in test_list:\n",
    "        \n",
    "        data=open(file,'r').read()      \n",
    "        wordscontent_in_data=re.split('[^a-zA-Z]',data)              \n",
    "        words_in_file=[]\n",
    "\n",
    "        for word in wordscontent_in_data:\n",
    "            if word not in stopWords:\n",
    "                if (len(word)>1):\n",
    "                    words_in_file.append(wnl.lemmatize(word.strip().lower(),\"v\"))\n",
    "                        \n",
    "        doc_dic={}\n",
    "        for w in words_in_file:\n",
    "            if w not in doc_dic:\n",
    "                doc_dic[w]=1\n",
    "                #print(doc_dic[w])\n",
    "            else:\n",
    "                doc_dic[w]=doc_dic[w]+1\n",
    "        \n",
    "        for wd in dic:\n",
    "            #print(wd)\n",
    "            if wd in doc_dic:\n",
    "                csv_file.write(\"%s,\"%(doc_dic[wd]))  \n",
    "                #print(\"saved in csv frq\"+str(doc_dic[wd]))\n",
    "            else:\n",
    "                csv_file.write(\"0,\")\n",
    "                #print(\"saved in csv frq 0\")\n",
    "        \n",
    "        if element=='comp':\n",
    "            csv_file.write(\"0\\n\")\n",
    "        elif element=='rec':\n",
    "            csv_file.write(\"1\\n\")\n",
    "        elif element=='sci':\n",
    "            csv_file.write(\"2\\n\")\n",
    "        elif element=='talk':\n",
    "            csv_file.write(\"3\\n\")\n",
    "        \n",
    "        #csv_file.write(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
