{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\comp\\comp.graphics\n",
      "train document count :680\n",
      "test  document count :291\n",
      ".\\data\\comp\\comp.os.ms-windows.misc\n",
      "train document count :688\n",
      "test  document count :295\n",
      ".\\data\\comp\\comp.sys.ibm.pc.hardware\n",
      "train document count :686\n",
      "test  document count :294\n",
      ".\\data\\comp\\comp.sys.mac.hardware\n",
      "train document count :673\n",
      "test  document count :288\n",
      ".\\data\\comp\\comp.windows.x\n",
      "train document count :690\n",
      "test  document count :296\n",
      "Total number of words in training set raw :1169154\n",
      "created train_comp.csv\n",
      "Total number of words in test set raw :864472\n",
      "created test_comp.csv\n",
      "**************************************new root class************************************************************\n",
      ".\\data\\rec\\rec.autos\n",
      "train document count :692\n",
      "test  document count :296\n",
      ".\\data\\rec\\rec.motorcycles\n",
      "train document count :696\n",
      "test  document count :298\n",
      ".\\data\\rec\\rec.sport.baseball\n",
      "train document count :694\n",
      "test  document count :298\n",
      ".\\data\\rec\\rec.sport.hockey\n",
      "train document count :698\n",
      "test  document count :299\n",
      "Total number of words in training set raw :1292859\n",
      "created train_rec.csv\n",
      "Total number of words in test set raw :542062\n",
      "created test_rec.csv\n",
      "**************************************new root class************************************************************\n",
      ".\\data\\sci\\sci.crypt\n",
      "train document count :692\n",
      "test  document count :297\n",
      ".\\data\\sci\\sci.electronics\n",
      "train document count :687\n",
      "test  document count :295\n",
      ".\\data\\sci\\sci.med\n",
      "train document count :692\n",
      "test  document count :296\n",
      ".\\data\\sci\\sci.space\n",
      "train document count :689\n",
      "test  document count :296\n",
      "Total number of words in training set raw :1352791\n",
      "created train_sci.csv\n",
      "Total number of words in test set raw :536550\n",
      "created test_sci.csv\n",
      "**************************************new root class************************************************************\n",
      ".\\data\\talk\\talk.politics.guns\n",
      "train document count :636\n",
      "test  document count :272\n",
      ".\\data\\talk\\talk.politics.mideast\n",
      "train document count :657\n",
      "test  document count :281\n",
      ".\\data\\talk\\talk.politics.misc\n",
      "train document count :541\n",
      "test  document count :232\n",
      ".\\data\\talk\\talk.religion.misc\n",
      "train document count :438\n",
      "test  document count :188\n",
      "Total number of words in training set raw :1048701\n",
      "created train_talk.csv\n",
      "Total number of words in test set raw :377196\n",
      "created test_talk.csv\n",
      "**************************************new root class************************************************************\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from random import shuffle\n",
    "root='.\\\\data'\n",
    "root_content=os.listdir(root)\n",
    "classes =root_content\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import re\n",
    "dic={}\n",
    "lemm =[]\n",
    "sentence =\"\"\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#################################################################################################\n",
    "def process_clustered_training(input_content,label) :\n",
    "    lemm =[]\n",
    "    sentence =\"\"\n",
    "    print(\"Total number of words in training set raw :\"+ str(len(input_content)))\n",
    "    words=re.split('[^a-zA-Z]',input_content)\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    new_word_list=[]\n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            if(len(w)>1):\n",
    "                new_word_list.append(w.strip().lower())\n",
    "    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmatize_list = [wnl.lemmatize(w,\"v\") for w in new_word_list]\n",
    "    \n",
    "    for word in lemmatize_list:\n",
    "        sentence=word+\" \"+sentence\n",
    "    \n",
    "    lemm.append(sentence)\n",
    "    \n",
    "    vector = CountVectorizer()\n",
    "    newvector = vector.fit_transform(lemm)\n",
    "    features = vector.get_feature_names()\n",
    "    \n",
    "    df = pd.DataFrame(newvector.toarray(), columns = features)\n",
    "    global namecounter\n",
    "    \n",
    "    df.to_csv(\"train_\"+str(label)+\".csv\")\n",
    "    print(\"created train_\"+str(label)+\".csv\")\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "\n",
    "\n",
    "def process_clustered_testing(input_content,label) :\n",
    "    lemm =[]\n",
    "    sentence =\"\"\n",
    "    print(\"Total number of words in test set raw :\"+ str(len(input_content)))\n",
    "    words=re.split('[^a-zA-Z]',input_content)\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    new_word_list=[]\n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            if(len(w)>1):\n",
    "                new_word_list.append(w.strip().lower())\n",
    "    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmatize_list = [wnl.lemmatize(w,\"v\") for w in new_word_list]\n",
    "    \n",
    "    for word in lemmatize_list:\n",
    "        sentence=word+\" \"+sentence\n",
    "    \n",
    "    lemm.append(sentence)\n",
    "    \n",
    "    vector = CountVectorizer()\n",
    "    newvector = vector.fit_transform(lemm)\n",
    "    features = vector.get_feature_names()\n",
    "    \n",
    "    df = pd.DataFrame(newvector.toarray(), columns = features)\n",
    "    global namecounter\n",
    "    df.to_csv(\"test_\"+str(label)+\".csv\")\n",
    "    print(\"created test_\"+str(label)+\".csv\")\n",
    "    \n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "for element in classes :\n",
    "    path=os.path.join(root,element)\n",
    "    sub_folders=os.listdir(path)\n",
    "    \n",
    "    file_label=os.path.basename(os.path.normpath(path))\n",
    "            \n",
    "    for sub_elem in sub_folders :\n",
    "        sub_path=os.path.join(path,sub_elem)\n",
    "        sub_folder_content=os.listdir(sub_path)\n",
    "        print(sub_path)\n",
    "        shuffle(sub_folder_content)\n",
    "        train_list = sub_folder_content[1 : int(len(sub_folder_content) * .7)]\n",
    "        test_list=sub_folder_content[int(len(sub_folder_content) * .7)+1 :int(len(sub_folder_content))]\n",
    "         \n",
    "        print(\"train document count :\"+str(len(train_list)))\n",
    "        print(\"test  document count :\"+str(len(test_list)))\n",
    "        file_count=0 \n",
    "        train_big_one = \"\"\n",
    "        test_big_one = \"\"\n",
    "        train_label=\"\"\n",
    "        test_label=\"\"\n",
    "        for file in train_list:\n",
    "            newpath=os.path.join(sub_path,file)\n",
    "            content=open(newpath,'r',encoding=\"utf-8\",errors='replace').read()\n",
    "            train_big_one+=content\n",
    "            train_label=os.path.basename(os.path.normpath(sub_path))\n",
    "            \n",
    "            \n",
    "    \n",
    "        for file in test_list: \n",
    "            newpath=os.path.join(sub_path,file)\n",
    "            content=open(newpath,'r',encoding=\"utf-8\",errors='replace').read()\n",
    "            test_big_one+=content\n",
    "            \n",
    "            \n",
    "    \n",
    "    process_clustered_training(train_big_one,file_label)\n",
    "    train_big_one=\"\"\n",
    "    \n",
    "    process_clustered_testing(test_big_one,file_label)\n",
    "    test_big_one=\"\"\n",
    "    \n",
    "        \n",
    "    print(\"**************************************new root class************************************************************\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
